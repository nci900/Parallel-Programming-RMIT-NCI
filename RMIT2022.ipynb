{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Parall Programming (RMIT-NCI 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# the jupyter notebook is launched from your $HOME, change the working directory provided a username directory is created under /scratch/vp91\n",
    "os.chdir(os.path.expandvars(\"/scratch/vp91/$USER/RMIT2022\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. OpenMP\n",
    "Our example ([monte-carlo-pi-serial](./monte-carlo-pi-serial.c)) for you to get a hang of  parallel programming is slightly more complicated than a helloword program. Nevertheless, it is a simple snippet showcasing a basic openmp program. \n",
    "\n",
    "The program approximates Pi by Monte-Carlo method. Run the next cell to compile and execute the serial code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f *.o laplace_mpi_blocking laplace_mpi_nonblocking laplace_mpi_persistent\n",
      "gcc -g -Wall -fopenmp -o monte-carlo-pi-serial monte-carlo-pi-serial.c -lm\n",
      "Compilation Successful!\n",
      "MATH Pi 3.141593\n",
      "/////////////////////////////////////////////////////\n",
      "Sampling points 4000000; Hit numbers 3140304; Approx Pi 3.140304, Total time in 0.064008 seconds \n",
      "Sampling points 8000000; Hit numbers 6282953; Approx Pi 3.141477, Total time in 0.102494 seconds \n",
      "Sampling points 16000000; Hit numbers 12565669; Approx Pi 3.141417, Total time in 0.204732 seconds \n",
      "Sampling points 32000000; Hit numbers 25130758; Approx Pi 3.141345, Total time in 0.410154 seconds \n",
      "Sampling points 64000000; Hit numbers 50264936; Approx Pi 3.141558, Total time in 0.818817 seconds \n",
      "Sampling points 128000000; Hit numbers 100532384; Approx Pi 3.141637, Total time in 1.640441 seconds \n",
      "Sampling points 256000000; Hit numbers 201059854; Approx Pi 3.141560, Total time in 3.281945 seconds \n",
      "Sampling points 512000000; Hit numbers 402124922; Approx Pi 3.141601, Total time in 6.557391 seconds \n",
      "Sampling points 1024000000; Hit numbers 804243492; Approx Pi 3.141576, Total time in 13.122737 seconds \n"
     ]
    }
   ],
   "source": [
    "!make clean && make mc-serial && echo \"Compilation Successful!\" && ./monte-carlo-pi-serial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multithreading version is implemented at ([monte-carlo-pi-openmp.c](./monte-carlo-pi-openmp.c)) by OpenMP. In essence, $N$ number of randowm numbers are distributed to multiple threads. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to compile the OpenMP code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f *.o laplace_mpi_blocking laplace_mpi_nonblocking laplace_mpi_persistent\n",
      "gcc  -g -fopenmp -Wall -o monte-carlo-pi-openmp monte-carlo-pi-openmp.c -lm\n",
      "Compilation Successful!\n"
     ]
    }
   ],
   "source": [
    "!make clean && make mc-omp && echo \"Compilation Successful!\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the program with a fixed number of threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATH Pi 3.141593\n",
      "/////////////////////////////////////////////////////\n",
      "Sampling points 4000000; Hit numbers 3142455; Approx Pi 3.142455, Total time in 0.015405 seconds \n",
      "Sampling points 8000000; Hit numbers 6284800; Approx Pi 3.142400, Total time in 0.016148 seconds \n",
      "Sampling points 16000000; Hit numbers 12566293; Approx Pi 3.141573, Total time in 0.019214 seconds \n",
      "Sampling points 32000000; Hit numbers 25133048; Approx Pi 3.141631, Total time in 0.035671 seconds \n",
      "Sampling points 64000000; Hit numbers 50264658; Approx Pi 3.141541, Total time in 0.071952 seconds \n",
      "Sampling points 128000000; Hit numbers 100525709; Approx Pi 3.141428, Total time in 0.143536 seconds \n",
      "Sampling points 256000000; Hit numbers 201053080; Approx Pi 3.141454, Total time in 0.285846 seconds \n",
      "Sampling points 512000000; Hit numbers 402112108; Approx Pi 3.141501, Total time in 0.572919 seconds \n"
     ]
    }
   ],
   "source": [
    "!OMP_NUM_THREADS=12 ./monte-carlo-pi-openmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. OpenACC\n",
    "Now we offload the computation to a GPU to accelerate the for-loop. To this end, firstly we need to load NVIDIA HPC STK module on Gadi.\n",
    "\n",
    "**`TODO`**: Refactor [monte-carlo-pi-openacc.c](./monte-carlo-pi-openacc.c) by changing to OpenACC clauses. \n",
    "\n",
    "Since we will compile with managed memory, there's no need to include data transfer clauses. But this will come to an issue for gaining more performance.\n",
    "\n",
    "The following flags are used in compiling the OpenACC code:\n",
    "\n",
    "-Minfo=accel: Show the information about the accelerated code by OpenACC\n",
    "\n",
    "-ta:telsa=mamaged: Target OpenACC to Nvidia GPUs with mamanged memory\n",
    "\n",
    "We also use NVTX libray which provides annotations for profiling the code.\n",
    "\n",
    "If you are getting stuck, peek the solution at ([solution](./solution/monte-carlo-pi-openacc.c))\n",
    "\n",
    "Once you have rendered the code with correct OpenACC, run the next cell to compile and execute the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f *.o laplace_mpi_blocking laplace_mpi_nonblocking laplace_mpi_persistent\n",
      "nvc -g -Wextra -acc -Minfo=accel -ta=tesla:managed  -o monte-carlo-pi-openacc monte-carlo-pi-openacc.c -lm -lnvToolsExt\n",
      "calc_pi:\n",
      "     39, Generating NVIDIA GPU code\n",
      "         39, #pragma acc loop gang, vector(128) /* blockIdx.x threadIdx.x */\n",
      "             Generating reduction(+:count)\n",
      "     39, Generating implicit copyin(random_array[:]) [if not already present]\n",
      "         Generating implicit copy(count) [if not already present]\n",
      "Compilation Successful!\n",
      "MATH Pi 3.141593\n",
      "Sampling points 4000000; Hit numbers 3141015; Approx Pi 3.141015, Total time in 0.240623 seconds \n",
      "Sampling points 8000000; Hit numbers 6281556; Approx Pi 3.140778, Total time in 0.158552 seconds \n",
      "Sampling points 16000000; Hit numbers 12565785; Approx Pi 3.141446, Total time in 0.301478 seconds \n",
      "Sampling points 32000000; Hit numbers 25129756; Approx Pi 3.141220, Total time in 0.587117 seconds \n",
      "Sampling points 64000000; Hit numbers 50263089; Approx Pi 3.141443, Total time in 1.155539 seconds \n",
      "Sampling points 128000000; Hit numbers 100529205; Approx Pi 3.141538, Total time in 2.288088 seconds \n",
      "Sampling points 256000000; Hit numbers 201065222; Approx Pi 3.141644, Total time in 4.552309 seconds \n",
      "Sampling points 512000000; Hit numbers 402124281; Approx Pi 3.141596, Total time in 9.069991 seconds \n"
     ]
    }
   ],
   "source": [
    "!make clean && make mc-acc && echo \"Compilation Successful!\" && ./monte-carlo-pi-openacc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will demonstrate how to submit a batch job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MPI\n",
    "Our last parallel programming model uses MPI. The total $N$ number of random numbers are split into multiple processors. Each process independtely calculates the number of random numbers that are locally stored witthin the process. The results of each individual MPI rank are collected and summed at a root process (MPI rank 0). \n",
    "\n",
    "Look out for the following parts in the program ([monte-carlo-pi-mpi.c](./monte-carlo-pi-mpi.c)).\n",
    "```cpp\n",
    "#include <mpi.h>\n",
    "\n",
    "MPI_Init(&argc, &argv);\n",
    "\n",
    "MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "\n",
    "MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "\n",
    "MPI_Wtime();\n",
    "\n",
    "MPI_Reduce(&count, &count_tot, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n",
    "\n",
    "MPI_Barrier(MPI_COMM_WORLD);\n",
    "\n",
    "MPI_Finalize();\n",
    "```\n",
    "\n",
    "Run the next cell to excute the MC_pi program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f *.o laplace_mpi_blocking laplace_mpi_nonblocking laplace_mpi_persistent\n",
      "mpicc -g -Wall -o monte-carlo-pi-mpi monte-carlo-pi-mpi.c -lmpiP -lm -lbfd -liberty -lunwind\n",
      "Compilation Successful!\n",
      "start 1000000 end 2000000 rank 1 \n",
      "start 2000000 end 3000000 rank 2 \n",
      "start 3000000 end 4000000 rank 3 \n",
      "mpiP: \n",
      "mpiP: mpiP: mpiP V3.4.1 (Build Apr  1 2020/12:09:46)\n",
      "mpiP: Direct questions and errors to mpip-help@lists.sourceforge.net\n",
      "mpiP: \n",
      "start 0 end 1000000 rank 0 \n",
      "MPI program runtime = 0.016044 on rank 1\n",
      "Hit numbers  3140429 Approx Pi 3.140429\n",
      "MPI program runtime = 0.018354 on rank 3\n",
      "MPI program runtime = 0.032232 on rank 2\n",
      "MPI program runtime = 0.030789 on rank 0\n",
      "mpiP: \n",
      "mpiP: Storing mpiP output in [./monte-carlo-pi-mpi.4.1968846.1.mpiP].\n",
      "mpiP: \n"
     ]
    }
   ],
   "source": [
    "!make clean && make mc-mpi && echo \"Compilation Successful!\" && mpiexec -np 4 ./monte-carlo-pi-mpi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profile with mpiP\n",
    "\n",
    "Run the next cell to inspect the profiling results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@ mpiP\n",
      "@ Command : ./monte-carlo-pi-mpi \n",
      "@ Version                  : 3.4.1\n",
      "@ MPIP Build date          : Apr  1 2020, 12:09:46\n",
      "@ Start time               : 2022 11 15 21:31:48\n",
      "@ Stop time                : 2022 11 15 21:31:48\n",
      "@ Timer Used               : PMPI_Wtime\n",
      "@ MPIP env var             : [null]\n",
      "@ Collector Rank           : 0\n",
      "@ Collector PID            : 1968846\n",
      "@ Final Output Dir         : .\n",
      "@ Report generation        : Single collector task\n",
      "@ MPI Task Assignment      : 0 gadi-gpu-v100-0090.gadi.nci.org.au\n",
      "@ MPI Task Assignment      : 1 gadi-gpu-v100-0090.gadi.nci.org.au\n",
      "@ MPI Task Assignment      : 2 gadi-gpu-v100-0090.gadi.nci.org.au\n",
      "@ MPI Task Assignment      : 3 gadi-gpu-v100-0090.gadi.nci.org.au\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "@--- MPI Time (seconds) ---------------------------------------------------\n",
      "---------------------------------------------------------------------------\n",
      "Task    AppTime    MPITime     MPI%\n",
      "   0     0.0398     0.0272    68.23\n",
      "   1     0.0323     0.0164    50.68\n",
      "   2     0.0411     0.0248    60.33\n",
      "   3     0.0411     0.0228    55.52\n",
      "   *      0.154     0.0911    59.07\n",
      "---------------------------------------------------------------------------\n",
      "@--- Callsites: 2 ---------------------------------------------------------\n",
      "---------------------------------------------------------------------------\n",
      " ID Lev File/Address          Line Parent_Funct             MPI_Call\n",
      "  1   0 monte-carlo-pi-mpi.c    59 main                     Reduce\n",
      "  2   0 monte-carlo-pi-mpi.c    68 main                     Barrier\n",
      "---------------------------------------------------------------------------\n",
      "@--- Aggregate Time (top twenty, descending, milliseconds) ----------------\n",
      "---------------------------------------------------------------------------\n",
      "Call                 Site       Time    App%    MPI%     COV\n",
      "Barrier                 2       56.8   36.80   62.29    0.47\n",
      "Reduce                  1       34.4   22.28   37.71    1.14\n",
      "---------------------------------------------------------------------------\n",
      "@--- Aggregate Sent Message Size (top twenty, descending, bytes) ----------\n",
      "---------------------------------------------------------------------------\n",
      "Call                 Site      Count      Total       Avrg  Sent%\n",
      "Reduce                  1          4         16          4 100.00\n",
      "---------------------------------------------------------------------------\n",
      "@--- Callsite Time statistics (all, milliseconds): 8 ----------------------\n",
      "---------------------------------------------------------------------------\n",
      "Name              Site Rank  Count      Max     Mean      Min   App%   MPI%\n",
      "Barrier              2    0      1     9.04     9.04     9.04  22.69  33.26\n",
      "Barrier              2    1      1     16.2     16.2     16.2  50.26  99.17\n",
      "Barrier              2    2      1     8.81     8.81     8.81  21.45  35.56\n",
      "Barrier              2    3      1     22.7     22.7     22.7  55.23  99.48\n",
      "Barrier              2    *      4     22.7     14.2     8.81  36.80  62.29\n",
      "\n",
      "Reduce               1    0      1     18.1     18.1     18.1  45.54  66.74\n",
      "Reduce               1    1      1    0.136    0.136    0.136   0.42   0.83\n",
      "Reduce               1    2      1       16       16       16  38.88  64.44\n",
      "Reduce               1    3      1    0.119    0.119    0.119   0.29   0.52\n",
      "Reduce               1    *      4     18.1     8.59    0.119  22.28  37.71\n",
      "---------------------------------------------------------------------------\n",
      "@--- Callsite Message Sent statistics (all, sent bytes) -------------------\n",
      "---------------------------------------------------------------------------\n",
      "Name              Site Rank   Count       Max      Mean       Min       Sum\n",
      "Reduce               1    0       1         4         4         4         4\n",
      "Reduce               1    1       1         4         4         4         4\n",
      "Reduce               1    2       1         4         4         4         4\n",
      "Reduce               1    3       1         4         4         4         4\n",
      "Reduce               1    *       4         4         4         4        16\n",
      "---------------------------------------------------------------------------\n",
      "@--- End of Report --------------------------------------------------------\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!cat *.mpiP\n",
    "!rm -r *.mpiP"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
